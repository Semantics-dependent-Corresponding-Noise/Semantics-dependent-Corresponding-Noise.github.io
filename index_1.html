<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Noise Datasets and Model Validation</title>
  
  <!-- Bulma CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <style>
    :root {
      --primary-color: #8B0000; /* 深红/红棕色 */
      --text-main: #2c3e50;
      --bg-light: #f8f9fa;
      --bg-section: #ffffff;
    }

    body {
      font-family: 'Poppins', sans-serif;
      color: var(--text-main);
      /* 基础字体大小适配 */
      font-size: clamp(1rem, 1.1vw, 1.2rem); 
      line-height: 1.8;
    }

    /* --- Typography Optimizations --- */
    h1, h2, h3, h4, h5, h6 {
      color: #000;
    }

    .hero-title {
      font-size: clamp(2rem, 5vw, 3.5rem); 
      font-weight: 700;
      line-height: 1.2;
    }
    
    .hero-subtitle {
      font-size: clamp(1.2rem, 3vw, 1.8rem);
      color: #4a4a4a;
      margin-top: 1.5rem;
      font-weight: 300;
    }

    .section-title {
      text-align: center;
      margin-bottom: 3rem;
    }
    
    .section-title h2 {
      color: var(--primary-color);
      font-size: clamp(2rem, 4vw, 2.5rem);
      font-weight: 600;
      position: relative;
      display: inline-block;
    }
    
    .section-title h2::after {
      content: '';
      display: block;
      width: 60px;
      height: 4px;
      background: var(--primary-color);
      margin: 10px auto 0;
      border-radius: 2px;
    }

    .content p, .content ul {
      text-align: justify; 
      margin-bottom: 1.5rem;
    }

    /* --- Component Styles --- */
    .hero { 
      background-color: var(--bg-light); 
      padding: 5rem 1.5rem; 
    }

    .content-section {
      padding: 4rem 0;
    }

    /* Table Styling */
    .table-container {
      box-shadow: 0 5px 15px rgba(0,0,0,0.05);
      border-radius: 8px;
      overflow: hidden;
    }
    .table thead th {
      background-color: var(--primary-color);
      color: white !important;
      font-weight: 600;
    }
    .table a {
      color: var(--primary-color);
      font-weight: 500;
      text-decoration: none;
      transition: color 0.2s;
    }
    .table a:hover {
      text-decoration: underline;
      color: #a52a2a;
    }

    /* --- Results Section Specifics --- */
    .result-box {
      background: #fff;
      border: 1px solid #eee;
      border-radius: 12px;
      padding: 2rem;
      height: 100%; 
      box-shadow: 0 4px 20px rgba(0,0,0,0.06);
      transition: transform 0.3s ease;
      display: flex;
      flex-direction: column;
    }
    
    .result-box:hover {
      transform: translateY(-5px);
      box-shadow: 0 8px 25px rgba(0,0,0,0.1);
    }

    .result-header {
      font-size: 1.5rem;
      color: var(--primary-color);
      font-weight: 700;
      margin-bottom: 1.5rem;
      border-bottom: 2px solid #f1f1f1;
      padding-bottom: 0.5rem;
    }

    .single-img-wrapper {
      margin-bottom: 1.5rem;
      border-radius: 8px;
      overflow: hidden;
      border: 1px solid #ddd;
      box-shadow: 0 2px 8px rgba(0,0,0,0.05);
    }
    
    .single-img-wrapper img {
      width: 100%;
      height: auto;
      display: block;
    }
    
    .img-label {
      font-size: 0.9rem;
      color: #666;
      text-align: center;
      margin-bottom: 0.5rem;
      font-weight: 500;
    }

    .footer-section {
      background: #2c3e50;
      color: white;
      padding: 4rem 0 2rem;
      margin-top: 4rem;
    }

    /* Mobile Tweaks */
    @media (max-width: 768px) {
      .hero { padding: 3rem 1rem; }
      .result-box { margin-bottom: 2rem; padding: 1.5rem; }
    }
  </style>
</head>
<body>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title hero-title">
              <img src="assets/img/book.png" style="width:1.2em; vertical-align: bottom; margin-right: 10px;" alt="Logo"/>
              <span style="color: #8B0000;">Semantic-dependent-Corresponding-Noise</span>
            </h1>
            <h2 class="subtitle hero-subtitle">
              Introduce the first benchmark targeting semantic-dependent noisy correspondence to reveal the limitations of random-shuffling evaluations
            </h2>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction Section -->
  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Introduction</h2>
      </div>
      <div class="content">
        <p>
          Vision-Language Pre-training relies on webcrawled datasets that inherently contain noisy image-text correspondences. While Noisy Correspondence Learning (NCL) addresses this issue, evaluation typically relies on random shuffled datasets, which fails to capture the structured, semantic-dependent biases observed in real data. We introduce the first benchmark targeting semantic-dependent noisy correspondence. On Flickr30k and MS-COCO, we synthesize four systematic noise types, namely object omission, short description, entity referential error, and high-level semantic confusion. For the first three noise, we use large multimodal and language models to produce controlled perturbations via (i) constrained image re-description and (ii) targeted caption editing. For semantic confusion, we exploit cluster structure in representation space to select plausible but incorrect substitutes. Re-evaluating leading NCL methods on our datasets reveals distinct and diagnostic robustness patterns across noise types, demonstrating that semantic-dependent benchmarks expose challenges that random-shuffling evaluations miss.
        </p>
        <figure class="image is-fullwidth" style="margin-top: 2rem;">
          <img src="assets/img/four_noise.png" alt="Noise Types" style="border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1);">
          <figcaption class="caption" style="text-align: center; font-size: 0.9em; color: #666; margin-top: 1rem; font-style: italic;">
            Figure 1: Framework for synthesizing structured semantic-dependent noise in image-text pairs.
          </figcaption>
        </figure>
      </div>
    </div>
  </section>

  <!-- Noise Datasets Table (Links Restored) -->
  <section class="content-section" style="background-color: #fcfcfc;">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Noise Datasets</h2>
      </div>
      <div class="content">
        <p>
          The dataset directory contains folders for each type of noise applied to the MS-COCO and Flickr30K datasets. The noise types include:
        </p>
        <ul>
          <li><strong>Entity Referential Error</strong>: Incorrect references to entities in captions.</li>
          <li><strong>High-level Semantic Confusion</strong>: Confusing high-level semantics in captions.</li>
          <li><strong>Object Omission</strong>: Important objects omitted from captions.</li>
          <li><strong>Short Description</strong>: Incomplete or overly brief captions.</li>
        </ul>
        <p>
          Each type of noise is available in two variants: <strong>5error</strong> (all five captions per image are noisy) and <strong>Mixed</strong> (noise is partially mixed).
        </p>
      </div>

      <div class="table-container">
        <table class="table is-fullwidth is-hoverable is-striped">
          <thead>
            <tr>
              <th style="width: 30%;">Dataset</th>
              <th>Flickr30k</th>
              <th>MS-COCO</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Object Omission</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_f30k" target="_blank">Object_Omission_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_MSCOCO" target="_blank">Object_Omission_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Object Omission (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_5error_f30k" target="_blank">Object_Omission_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_5error_MSCOCO" target="_blank">Object_Omission_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Short Description</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_f30k" target="_blank">Short_Description_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_MSCOCO" target="_blank">Short_Description_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Short Description (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_5error_f30k" target="_blank">Short_Description_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_5error_MSCOCO" target="_blank">Short_Description_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Entity Referential Error</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_f30k" target="_blank">Entity_Referential_Error_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_MSCOCO" target="_blank">Entity_Referential_Error_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Entity Referential Error (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_5error_f30k" target="_blank">Entity_Referential_Error_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_5error_MSCOCO" target="_blank">Entity_Referential_Error_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>High-level Semantic Confusion</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_f30k" target="_blank">High_level_Semantic_Confusion_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_MSCOCO" target="_blank">High_level_Semantic_Confusion_MSCOCO</a></td>
            </tr>
            <tr>
              <td>High-level Semantic Confusion (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_5error_f30k" target="_blank">High_level_Semantic_Confusion_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_5error_MSCOCO" target="_blank">High_level_Semantic_Confusion_5error_MSCOCO</a></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>
  </section>

  <!-- Model Training Methodologies -->
  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Model Training Methodologies</h2>
      </div>
      <div class="content">
        <p>
          We validate the effectiveness of the noise datasets using three primary training strategies:
        </p>
        <ul>
          <li><strong>CLIP Pre-trained Model</strong>: Fine-tuning a pre-trained CLIP model to evaluate direct performance degradation under noisy conditions.</li>
          <li><strong>NPC (Noise Pre-training and Class-wise fine-tuning)</strong>: Pre-training on noisy datasets followed by class-wise fine-tuning to assess robustness.</li>
          <li><strong>GLP (Gradient-based Label Propagation)</strong>: Applying gradient-based label propagation to mitigate the effects of incorrect semantic alignments.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- Results Analysis Section -->
  <section class="content-section" style="background-color: #f0f2f5;">
    <div class="container is-max-widescreen">
      <div class="section-title">
        <h2>Results Analysis</h2>
      </div>
      
      <div class="columns is-variable is-8">
        
        <!-- Module 1: Robustness Comparison (Two Images) -->
        <div class="column is-half">
          <div class="result-box">
            <h3 class="result-header">Robustness Comparison: NPC vs. CLIP</h3>
            
            <!-- 第一张图片 (Table 1) -->
            <div class="img-label">Table 1: Retrieval metrics for Image-to-Text and Text-to-Image on Flickr30K with various noise (CLIP / NPC)</div>
            <div class="single-img-wrapper">
               <img src="assets/img/table1.png" alt="Comparison Table on Flickr30k">
            </div>

            <!-- 第二张图片 (Table 2) -->
            <div class="img-label">Table 2: Retrieval metrics for Image-to-Text and Text-to-Image on MSCOCO 5K with various noise(CLIP / NPC)</div>
            <div class="single-img-wrapper">
               <img src="assets/img/table2.png" alt="Comparison Table on MS-COCO">
            </div>

            <div class="content">
              <p>
                Our comparisons on <strong>Flickr30k</strong> and <strong>MS-COCO</strong> reveal that <strong>NPC</strong> outperforms <strong>CLIP</strong> in noise robustness. Compared to random shuffling (RS), NPC demonstrates significantly greater robustness under semantic noise (especially <strong>SD/SC</strong>), with this advantage increasing as the noise rate rises. 
              </p>
              <p>
                Although object omission (<strong>OO</strong>) poses the most severe disruption, limiting NPC's improvement, the overall difficulty ranking (<strong>RS < SC≈SD < ER < OO</strong>) remains stable across datasets. The findings indicate that RS underestimates NPC's true capability in handling structural semantic mismatches (rather than simple permutations).
              </p>
            </div>
          </div>
        </div>

        <!-- Module 2: Noise Impact & Bias (One Image) -->
        <div class="column is-half">
          <div class="result-box">
            <h3 class="result-header">Noise Impact on Model Behavior</h3>
            
            <!-- 第三张图片 (Fig 3) -->
            <div class="single-img-wrapper">
              <img src="assets/img/figure2.png" alt="Average Similarity Score Analysis (Fig. 2)">
              <div class="img-label">Figure 2: Average Cross-Modal Similarity Scores of
                Noisily Trained Models on Semantic-Dependent Noisy
                Test Sets</div>
            </div>

            <div class="content">
              <p>
                To measure how noise during training shapes model behavior, we evaluate each noisily trained model by computing its average similarity score across pairs in corrupted test sets (and a clean test set as a baseline). For each model-dataset pair, we first generate similarity outputs for all pairs in a given (noisy or clean) test set, then aggregate these values to obtain a single average score.
              </p>
              <p>
                The results (visualized in <strong>Fig. 2</strong>) reveal a consistent pattern: models trained on a specific type of noisy data tend to exhibit elevated similarity scores when evaluated on test sets with the same noise type. It shows that noisily trained models inherit the semantic biases of their training data, suggesting that the latent biases present in the real-world training dataset are passed on to the model itself.
              </p>
            </div>
          </div>
        </div>

      </div> <!-- End Columns -->
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer-section">
    <div class="container">
      <div class="columns">
        <div class="column is-half">
          <h3 style="color: #ff8b8b; font-size: 1.4rem; margin-bottom: 1rem;">Noise Datasets</h3>
          <p style="opacity: 0.8;">Exploring the Impact of Noise Datasets on Model Training and Validation</p>
        </div>
        <div class="column is-half has-text-right-tablet">
          <h4 style="color: #ff8b8b; margin-bottom: 1rem;">Links</h4>
          <ul style="list-style: none; padding: 0;">
            <li style="margin-bottom: 0.5rem;"><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise" style="color: white; opacity: 0.8; text-decoration: none;">GitHub Repository</a></li>
          </ul>
        </div>
      </div>
      <hr style="border-color: rgba(255,255,255,0.1); margin: 2rem 0 1rem;">
      <div class="has-text-centered">
        <p style="opacity: 0.6; font-size: 0.9rem;">&copy; 2025 Noise Datasets and Model Validation. All Rights Reserved.</p>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>