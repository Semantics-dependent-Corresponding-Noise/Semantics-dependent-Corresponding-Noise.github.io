<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Semantics-dependent Corresponding Noise</title>
  
  <!-- Bulma CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <style>
    :root {
      --primary-color: #8B0000;
      --text-main: #2c3e50;
      --bg-light: #f8f9fa;
    }

    body {
      font-family: 'Poppins', sans-serif;
      color: var(--text-main);
      font-size: 1rem; 
      line-height: 1.7;
    }

    .container.is-max-desktop {
      max-width: 960px !important; 
    }

    /* --- Typography --- */
    h1, h2, h3 { color: #000; }

    .hero-title {
      font-size: 2.5rem; 
      font-weight: 700;
      line-height: 1.2;
    }
    
    .hero-subtitle {
      font-size: 1.2rem;
      color: #4a4a4a;
      margin-top: 1rem;
      font-weight: 300;
    }

    .section-title {
      text-align: center;
      margin-bottom: 2.5rem;
    }
    
    .section-title h2 {
      color: var(--primary-color);
      font-size: 2rem; 
      font-weight: 600;
      position: relative;
      display: inline-block;
    }
    
    .section-title h2::after {
      content: '';
      display: block;
      width: 50px;
      height: 3px;
      background: var(--primary-color);
      margin: 8px auto 0;
      border-radius: 2px;
    }

    .content p, .content ul {
      text-align: justify; 
      margin-bottom: 1.2rem;
    }

    /* --- Sections --- */
    .hero { 
      background-color: var(--bg-light); 
      padding: 4rem 1.5rem; 
    }
    .content-section { padding: 3rem 0; }

    /* Table Styling */
    .table-container {
      box-shadow: 0 4px 10px rgba(0,0,0,0.05);
      border-radius: 8px;
      overflow: hidden;
      font-size: 0.9rem; 
    }
    .table thead th {
      background-color: var(--primary-color);
      color: white !important;
      font-weight: 600;
    }
    .table a {
      color: var(--primary-color);
      font-weight: 500;
      text-decoration: none;
      word-break: break-all; 
    }
    .table a:hover {
      text-decoration: underline;
      color: #a52a2a;
    }

    /* --- Result Box --- */
    .result-section-wrapper {
      background-color: #f4f6f8;
    }
    
    .result-card {
      background: #fff;
      border-radius: 12px;
      padding: 2rem;
      margin-bottom: 2.5rem; 
      box-shadow: 0 4px 20px rgba(0,0,0,0.06);
      border: 1px solid #eaeaea;
    }

    .result-header {
      font-size: 1.4rem;
      color: var(--primary-color);
      font-weight: 700;
      margin-bottom: 1.2rem;
      border-left: 4px solid var(--primary-color);
      padding-left: 12px;
    }

    .large-img-container {
      margin: 1.5rem 0;
      border-radius: 8px;
      overflow: hidden;
      border: 1px solid #ddd;
      box-shadow: 0 3px 8px rgba(0,0,0,0.05);
    }
    
    .img-caption-tag {
      background: #f1f1f1;
      color: #333; 
      padding: 8px 15px; 
      font-size: 1rem; 
      font-weight: 600; 
      text-align: center;
      border-bottom: 1px solid #ddd;
    }

    .footer-section {
      background: #2c3e50;
      color: white;
      padding: 3rem 0 1.5rem;
      margin-top: 3rem;
    }

    /* Mobile Tweaks */
    @media (max-width: 768px) {
      .hero { padding: 3rem 1rem; }
      .result-card { padding: 1.5rem; }
      .hero-title { font-size: 1.8rem; }
    }

    /* --- New Logo Section --- */
    .logo-section {
      display: flex;
      justify-content: center;
      gap: 2rem;
      margin-top: 2rem;
      flex-wrap: wrap;
    }
    
    .logo-item {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 0.5rem;
      transition: transform 0.3s ease;
    }
    
    .logo-item:hover {
      transform: translateY(-5px);
    }
    
    .logo-icon {
      font-size: 2.5rem;
      color: var(--primary-color);
      transition: color 0.3s ease;
    }
    
    .logo-item:hover .logo-icon {
      color: #a52a2a;
    }
    
    .logo-text {
      font-size: 0.9rem;
      font-weight: 500;
      color: var(--text-main);
    }

    /* --- New Test Dataset Section --- */
    .test-dataset-section {
      background-color: #fcfcfc;
      padding: 3rem 0;
    }
    
    .test-dataset-card {
      background: #fff;
      border-radius: 12px;
      padding: 2rem;
      margin-bottom: 2rem;
      box-shadow: 0 4px 20px rgba(0,0,0,0.06);
      border: 1px solid #eaeaea;
    }
    
    .test-dataset-header {
      font-size: 1.4rem;
      color: var(--primary-color);
      font-weight: 700;
      margin-bottom: 1.2rem;
      border-left: 4px solid var(--primary-color);
      padding-left: 12px;
    }
    
    .test-dataset-links {
      display: flex;
      flex-direction: column;
      gap: 1rem;
    }
    
    .test-dataset-link {
      display: flex;
      align-items: center;
      gap: 1rem;
      padding: 1rem;
      background: #f8f9fa;
      border-radius: 8px;
      transition: background 0.3s ease;
    }
    
    .test-dataset-link:hover {
      background: #e9ecef;
    }
    
    .test-dataset-link-icon {
      font-size: 1.5rem;
      color: var(--primary-color);
    }
    
    .test-dataset-link-text {
      font-size: 1rem;
      font-weight: 500;
      color: var(--text-main);
    }
    
    .test-dataset-link a {
      color: var(--primary-color);
      font-weight: 500;
      text-decoration: none;
      word-break: break-all;
    }
    
    .test-dataset-link a:hover {
      text-decoration: underline;
      color: #a52a2a;
    }

    /* Image gallery styles */
    .gallery {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 15px;
    }

    .gallery img {
      width: 100%;
      height: auto;
      border-radius: 5px;
    }

    .caption {
      font-size: 14px;
      margin-top: 5px;
    }
  </style>
</head>
<body>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title hero-title">
              <img src="assets/img/book.png" style="width:1.2em; vertical-align: bottom; margin-right: 10px;" alt="Logo"/>
              <span style="color: #8B0000;">Semantic-dependent-Corresponding-Noise</span>
            </h1>
            <h2 class="subtitle hero-subtitle">
              Introduce the first benchmark targeting semantics-dependent noisy correspondence to reveal the limitations of random-shuffling evaluations
            </h2>
            
            <!-- New Logo Section -->
            <div class="logo-section">
              <a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise" target="_blank" class="logo-item">
                <i class="fab fa-github logo-icon"></i>
                <span class="logo-text">GitHub Repository</span>
              </a>
              <a href="#dataset-table" class="logo-item">
                <i class="fas fa-database logo-icon"></i>
                <span class="logo-text">Training Datasets</span>
              </a>
              <a href="#test-datasets" class="logo-item">
                <i class="fas fa-file-alt logo-icon"></i>
                <span class="logo-text">Test Datasets</span>
              </a>
              <a href="#data-samples" class="logo-item">
                <i class="fas fa-image logo-icon"></i>
                <span class="logo-text">Data Samples</span>
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction Section -->
  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Introduction</h2>
      </div>
      <div class="content">
        <p>
          Cross-modal retrieval faces significant challenges from <strong>noisy web-crawled data</strong>, yet current <strong>Noisy Correspondence Learning (NCL) evaluation protocols</strong> rely on unrealistic uniform noise assumptions like <strong>random shuffling</strong>, which fail to capture the structured, <strong>semantics-dependent biases</strong> inherent in real-world mismatches. Unlike <strong>Noisy Label Learning</strong>, which rigorously distinguishes between uniform and instance-dependent errors, NCL has largely overlooked how <strong>systematic annotation and collection biases</strong> create content-specific noise.
        </p>
        <p>
          To bridge this gap, we introduce a <strong>novel benchmark and generation framework</strong>, as illustrated in <strong>Fig. 1</strong>, that leverages <strong>Large Multimodal and Language Models (LMMs and LLMs)</strong> to synthesize <strong>controlled, realistic, and semantics-dependent perturbations</strong>. This approach moves beyond random shuffling to provide a <strong>rigorous evaluation of model robustness</strong> against complex data corruption.
        </p>
        <figure class="image is-fullwidth" style="margin-top: 2rem;">
          <img src="assets/img/four_noise.png" alt="Noise Types" style="border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1);">
          <div class="img-caption-tag">Figure 1: Framework for synthesizing structured semantics-dependent noise in image-text pairs.</div>
        </figure>
      </div>
    </div>
  </section>

  <!-- Image and Caption Section -->
  <section class="content-section"  id="data-samples">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Image and Caption Visualization</h2>
      </div>
      <div class="gallery">
        <div class="item">
          <img src="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise.github.io/raw/main/assets/img/selected_images/1084040636.jpg" alt="1084040636">
          <div class="caption">
            <strong>Clean:</strong> A white dog is resting its head on a tiled floor with its eyes open.<br>
            <strong>Object Omission:</strong> A cool, green tiled floor with a stone-like pattern.<br>
            <strong>Entity Referential Error:</strong> A black cat is resting its head on a wooden floor with its eyes open.<br>
            <strong>Short Description:</strong> A white dog rests its head on the patio.<br>
            <strong>High-level Semantic Confusion:</strong> A couple bows their head as a man in a decorative robe reads from a scroll in Asia with a black late model station wagon in the background.
          </div>
        </div>
        <div class="item">
          <img src="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise.github.io/raw/main/assets/img/selected_images/1295719054.jpg" alt="1295719054">
          <div class="caption">
            <strong>Clean:</strong> Kids running to the playground.<br>
            <strong>Object Omission:</strong> A pathway surrounded by plants and bushes.<br>
            <strong>Entity Referential Error:</strong> A child in a red cape is being chased by other children.<br>
            <strong>Short Description:</strong> Four boys run down a sidewalk.<br>
            <strong>High-level Semantic Confusion:</strong> Two children play on large barrel hoops on a playground.
          </div>
        </div>
        <div class="item">
          <img src="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise.github.io/raw/main/assets/img/selected_images/1306038218.jpg" alt="1306038218">
          <div class="caption">
            <strong>Clean:</strong> Two soccer players are going for the ball.<br>
            <strong>Object Omission:</strong> Soccer ball, bleachers, grassy area.<br>
            <strong>Entity Referential Error:</strong> Two basketball players are going for the basketball.<br>
            <strong>Short Description:</strong> Two soccer players.<br>
            <strong>High-level Semantic Confusion:</strong> A group of senior citizens are watching a performance.
          </div>
        </div>
        <!-- <div class="item">
          <img src="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise.github.io/raw/main/assets/img/selected_images/1336772621.jpg" alt="1336772621">
          <div class="caption">
            <strong>Clean:</strong> An overweight woman prepares to skin a pineapple.<br>
            <strong>Object Omission:</strong> Trees, a house, a black bucket of fruits.<br>
            <strong>Entity Referential Error:</strong> An overweight man prepares to skin a pineapple.<br>
            <strong>Short Description:</strong> A woman in a blue dress.<br>
            <strong>High-level Semantic Confusion:</strong> A woman in a dress is holding something she has caught on the beach.
          </div>
        </div> -->
        <div class="item">
          <img src="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise.github.io/raw/main/assets/img/selected_images/1354131040.jpg" alt="1354131040">
          <div class="caption">
            <strong>Clean:</strong> A middle-aged gentleman is sitting in a small booth.<br>
            <strong>Object Omission:</strong> Two tables filled with fish, a scale, and bowls.<br>
            <strong>Entity Referential Error:</strong> A fisherman with fresh produce.<br>
            <strong>Short Description:</strong> A middle-aged gentleman in a booth.<br>
            <strong>High-level Semantic Confusion:</strong> A man and a woman fish from a boat.
          </div>
        </div>
        <div class="item">
          <img src="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise.github.io/raw/main/assets/img/selected_images/206051614.jpg" alt="206051614">
          <div class="caption">
            <strong>Clean:</strong> A man with brown hair is holding a telephone.<br>
            <strong>Object Omission:</strong> A workspace with a keyboard.<br>
            <strong>Entity Referential Error:</strong> A woman talking on the phone.<br>
            <strong>Short Description:</strong> A man leaning on a desk.<br>
            <strong>High-level Semantic Confusion:</strong> People making international phone calls.
          </div>
        </div>
        <div class="item">
          <img src="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise.github.io/raw/main/assets/img/selected_images/2095007523.jpg" alt="2095007523">
          <div class="caption">
            <strong>Clean:</strong> A baby, surrounded by some toys.<br>
            <strong>Object Omission:</strong> Colorful toys and a white blanket.<br>
            <strong>Entity Referential Error:</strong> A toddler looking through crib bars at camera.<br>
            <strong>Short Description:</strong> A baby surrounded by toys.<br>
            <strong>High-level Semantic Confusion:</strong> A woman fishing while a man turns to face the camera.
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Noise Datasets Table -->
  <section class="content-section" style="background-color: #fcfcfc;" id="dataset-table">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Noise Datasets</h2>
      </div>
      <div class="content">
        <p>
          The dataset directory contains folders for each type of noise applied to the MS-COCO and Flickr30K datasets. The noise types include:
        </p>
        <ul>
          <li><strong>Entity Referential Error (ER)</strong>: Specific entities in the captions are replaced with semantically related but incorrect terms, creating fine-grained mismatches.</li>
          <li><strong>High-Level Semantic Confusion (SC)</strong>: Captions are swapped from semantically proximate images in feature space.</li>
          <li><strong>Object Omission (OO)</strong>: Captions describe the scene but omit some objects, simulating incomplete annotation.</li>
          <li><strong>Short Description (SD)</strong>: Captions are condensed to generic statements, mimicking low-effort annotations.</li>
        </ul>
        <p>
         To download the captions, first visit:
        </p>
      </div>

      <div class="table-container">
        <table class="table is-fullwidth is-hoverable is-striped">
          <thead>
            <tr>
              <th style="width: 25%;">Dataset</th>
              <th>Flickr30k</th>
              <th>MS-COCO</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Object Omission</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_f30k/annotations/scan_split/" target="_blank">Object_Omission_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_MSCOCO/annotations/scan_split/" target="_blank">Object_Omission_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Object Omission (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_5error_f30k/annotations/scan_split/" target="_blank">Object_Omission_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_5error_MSCOCO/annotations/scan_split/" target="_blank">Object_Omission_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Short Description</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_f30k/annotations/scan_split/" target="_blank">Short_Description_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_MSCOCO/annotations/scan_split/" target="_blank">Short_Description_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Short Description (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_5error_f30k/annotations/scan_split/" target="_blank">Short_Description_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_5error_MSCOCO/annotations/scan_split/" target="_blank">Short_Description_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Entity Referential Error</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_f30k/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_MSCOCO/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Entity Referential Error (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_5error_f30k/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_5error_MSCOCO/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>High-level Semantic Confusion</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_f30k/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_MSCOCO/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_MSCOCO</a></td>
            </tr>
            <tr>
              <td>High-level Semantic Confusion (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_5error_f30k/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_5error_MSCOCO/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_5error_MSCOCO</a></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>
        <p>Then you can access the training files with different noise ratios according to your needs: 0.2-1.0_noise_train_caps.txt </p>
      </p>
    </div>
  </section>

  <!-- New Test Dataset Section -->
  <section class="test-dataset-section" id="test-datasets">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Noisy Test Datasets</h2>
      </div>
      
      <div class="test-dataset-card">
        <h3 class="test-dataset-header">Noisy Test Captions</h3>
        
        <div class="test-dataset-links">
          <div class="test-dataset-link">
            <i class="fas fa-file-alt test-dataset-link-icon"></i>
            <div class="test-dataset-link-text">
              MS-COCO Noisy Test Captions: 
              <a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/blob/main/dataset/test_caps_mix_coco.txt" target="_blank">test_caps_mix_coco.txt</a>
            </div>
          </div>
          
          <div class="test-dataset-link">
            <i class="fas fa-file-alt test-dataset-link-icon"></i>
            <div class="test-dataset-link-text">
              Flickr30K Noisy Test Captions: 
              <a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/blob/main/dataset/test_caps_mix_f30k.txt" target="_blank">test_caps_mix_f30k.txt</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Model Training Methodologies -->
  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Model Training Methodologies</h2>
      </div>
      <div class="content">
        <p>
          We validate the effectiveness of the noise datasets using two primary training strategies:
        </p>
        <ul>
          <li><strong>CLIP Pre-trained Model</strong>: Fine-tuning a pre-trained CLIP model to evaluate direct performance degradation under noisy conditions.</li>
          <li><strong>NPC (Negative Pre-aware for Noisy Cross-modal Matching)</strong>: Estimates sample-specific negative impact via a clean-sample memory bank, adaptively weighting training samples to prevent noise accumulation and ensure stable high-noise performance.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- Results Analysis Section -->
  <section class="content-section result-section-wrapper">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Results Analysis</h2>
      </div>
      
      <!-- Module 1: Robustness Comparison -->
      <div class="result-card">
        <h3 class="result-header">1. Robustness Comparison: NPC vs. CLIP</h3>
        
        <div class="content">
          <p>
            Our comparisons on <strong>Flickr30k</strong> and <strong>MS-COCO</strong>. 
          </p>
        </div>

        <div class="columns is-variable is-6">
          <div class="column is-half">
            <div class="large-img-container">
               <div class="img-caption-tag">Table 1: Retrieval metrics for Image-to-Text and Text-to-Image on Flickr30K with various noise (CLIP / NPC)</div>
               <img src="assets/img/table1.png" alt="Comparison Table on Flickr30k">
            </div>
          </div>
          <div class="column is-half">
            <div class="large-img-container">
               <div class="img-caption-tag">Table 2: Retrieval metrics for Image-to-Text and Text-to-Image on MSCOCO 5K with various noise(CLIP / NPC)</div>
               <img src="assets/img/table2.png" alt="Comparison Table on MS-COCO">
            </div>
          </div>
        </div>

        <div class="content">

        </div>
      </div>



    </div>
  </section>

  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Conclusion</h2>
      </div>
      <div class="content">
        We introduced a semantics-aware benchmark and generation framework for evaluating robustness in noisy imageâ€“text correspondence, moving beyond random shuffling to four systematic, semantics-dependent noise types. Through controlled LMM/LLM re-captioning and feature-space retrieval, we created reproducible perturbations with distinct linguistic and distributional signatures that expose complementary failure modes in cross-modal retrieval.
        Our re-evaluation of representative baselines and NCL methods reveals several key findings: Robustness under random shuffling overestimates true resilience. NCL strategies tuned for unstructured corruption can underperform simple fine-tuning when residual semantic cues persist, especially at high noise ratios, highlighting the need for denoising that preserves weak but useful alignment signals rather than suppressing them.
        Together, these results argue for semantics-aware evaluation as a necessary complement to existing benchmarks. Our benchmark provides a practical, extensible testbed to study structured correspondence noise, enabling more faithful measurement and targeted algorithmic advances.  We envision this benchmark could facilitate a shift toward methods and metrics that match the semantic reality of noisy web-scale supervision.
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer-section">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column is-half">
          <h3 style="color: #ff8b8b; font-size: 1.4rem; margin-bottom: 1rem;">Semantic-dependent-Corresponding-Noise</h3>
          <p style="opacity: 0.8;">Introduce the first benchmark targeting semantics-dependent noisy correspondence to reveal the limitations of random-shuffling evaluations</p>
        </div>
        <div class="column is-half has-text-right-tablet">
          <h4 style="color: #ff8b8b; margin-bottom: 1rem;">Links</h4>
          <ul style="list-style: none; padding: 0;">
            <li style="margin-bottom: 0.5rem;"><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise" style="color: white; opacity: 0.8; text-decoration: none;">GitHub Repository</a></li>
          </ul>
        </div>
      </div>
      <hr style="border-color: rgba(255,255,255,0.1); margin: 2rem 0 1rem;">
      <div class="has-text-centered">
        <p style="opacity: 0.6; font-size: 0.9rem;">&copy; 2025 Semantic-dependent-Corresponding-Noise. All Rights Reserved.</p>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
