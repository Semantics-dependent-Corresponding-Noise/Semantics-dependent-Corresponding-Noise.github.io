<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Semantic-dependent Corresponding Noise</title>
  
  <!-- Bulma CSS -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <!-- Bootstrap CSS -->
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <!-- Google Fonts -->
  <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">

  <style>
    :root {
      --primary-color: #8B0000;
      --text-main: #2c3e50;
      --bg-light: #f8f9fa;
    }

    body {
      font-family: 'Poppins', sans-serif;
      color: var(--text-main);
      font-size: 1rem; 
      line-height: 1.7;
    }

    .container.is-max-desktop {
      max-width: 960px !important; 
    }

    /* --- Typography --- */
    h1, h2, h3 { color: #000; }

    .hero-title {
      font-size: 2.5rem; 
      font-weight: 700;
      line-height: 1.2;
    }
    
    .hero-subtitle {
      font-size: 1.2rem;
      color: #4a4a4a;
      margin-top: 1rem;
      font-weight: 300;
    }

    .section-title {
      text-align: center;
      margin-bottom: 2.5rem;
    }
    
    .section-title h2 {
      color: var(--primary-color);
      font-size: 2rem; 
      font-weight: 600;
      position: relative;
      display: inline-block;
    }
    
    .section-title h2::after {
      content: '';
      display: block;
      width: 50px;
      height: 3px;
      background: var(--primary-color);
      margin: 8px auto 0;
      border-radius: 2px;
    }

    .content p, .content ul {
      text-align: justify; 
      margin-bottom: 1.2rem;
    }

    /* --- Sections --- */
    .hero { 
      background-color: var(--bg-light); 
      padding: 4rem 1.5rem; 
    }
    .content-section { padding: 3rem 0; }

    /* Table Styling */
    .table-container {
      box-shadow: 0 4px 10px rgba(0,0,0,0.05);
      border-radius: 8px;
      overflow: hidden;
      font-size: 0.9rem; 
    }
    .table thead th {
      background-color: var(--primary-color);
      color: white !important;
      font-weight: 600;
    }
    .table a {
      color: var(--primary-color);
      font-weight: 500;
      text-decoration: none;
      word-break: break-all; 
    }
    .table a:hover {
      text-decoration: underline;
      color: #a52a2a;
    }

    /* --- Result Box --- */
    .result-section-wrapper {
      background-color: #f4f6f8;
    }
    
    .result-card {
      background: #fff;
      border-radius: 12px;
      padding: 2rem;
      margin-bottom: 2.5rem; 
      box-shadow: 0 4px 20px rgba(0,0,0,0.06);
      border: 1px solid #eaeaea;
    }

    .result-header {
      font-size: 1.4rem;
      color: var(--primary-color);
      font-weight: 700;
      margin-bottom: 1.2rem;
      border-left: 4px solid var(--primary-color);
      padding-left: 12px;
    }

    .large-img-container {
      margin: 1.5rem 0;
      border-radius: 8px;
      overflow: hidden;
      border: 1px solid #ddd;
      box-shadow: 0 3px 8px rgba(0,0,0,0.05);
    }
    
    .img-caption-tag {
      background: #f1f1f1;
      color: #333; 
      padding: 8px 15px; 
      font-size: 1rem; 
      font-weight: 600; 
      text-align: center;
      border-bottom: 1px solid #ddd;
    }

    .footer-section {
      background: #2c3e50;
      color: white;
      padding: 3rem 0 1.5rem;
      margin-top: 3rem;
    }

    /* Mobile Tweaks */
    @media (max-width: 768px) {
      .hero { padding: 3rem 1rem; }
      .result-card { padding: 1.5rem; }
      .hero-title { font-size: 1.8rem; }
    }

    /* --- New Logo Section --- */
    .logo-section {
      display: flex;
      justify-content: center;
      gap: 2rem;
      margin-top: 2rem;
      flex-wrap: wrap;
    }
    
    .logo-item {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 0.5rem;
      transition: transform 0.3s ease;
    }
    
    .logo-item:hover {
      transform: translateY(-5px);
    }
    
    .logo-icon {
      font-size: 2.5rem;
      color: var(--primary-color);
      transition: color 0.3s ease;
    }
    
    .logo-item:hover .logo-icon {
      color: #a52a2a;
    }
    
    .logo-text {
      font-size: 0.9rem;
      font-weight: 500;
      color: var(--text-main);
    }

    /* --- New Test Dataset Section --- */
    .test-dataset-section {
      background-color: #fcfcfc;
      padding: 3rem 0;
    }
    
    .test-dataset-card {
      background: #fff;
      border-radius: 12px;
      padding: 2rem;
      margin-bottom: 2rem;
      box-shadow: 0 4px 20px rgba(0,0,0,0.06);
      border: 1px solid #eaeaea;
    }
    
    .test-dataset-header {
      font-size: 1.4rem;
      color: var(--primary-color);
      font-weight: 700;
      margin-bottom: 1.2rem;
      border-left: 4px solid var(--primary-color);
      padding-left: 12px;
    }
    
    .test-dataset-links {
      display: flex;
      flex-direction: column;
      gap: 1rem;
    }
    
    .test-dataset-link {
      display: flex;
      align-items: center;
      gap: 1rem;
      padding: 1rem;
      background: #f8f9fa;
      border-radius: 8px;
      transition: background 0.3s ease;
    }
    
    .test-dataset-link:hover {
      background: #e9ecef;
    }
    
    .test-dataset-link-icon {
      font-size: 1.5rem;
      color: var(--primary-color);
    }
    
    .test-dataset-link-text {
      font-size: 1rem;
      font-weight: 500;
      color: var(--text-main);
    }
    
    .test-dataset-link a {
      color: var(--primary-color);
      font-weight: 500;
      text-decoration: none;
      word-break: break-all;
    }
    
    .test-dataset-link a:hover {
      text-decoration: underline;
      color: #a52a2a;
    }
  </style>
</head>
<body>

  <!-- Hero Section -->
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title hero-title">
              <img src="assets/img/book.png" style="width:1.2em; vertical-align: bottom; margin-right: 10px;" alt="Logo"/>
              <span style="color: #8B0000;">Semantic-dependent-Corresponding-Noise</span>
            </h1>
            <h2 class="subtitle hero-subtitle">
              Introduce the first benchmark targeting semantic-dependent noisy correspondence to reveal the limitations of random-shuffling evaluations
            </h2>
            
            <!-- New Logo Section -->
            <div class="logo-section">
              <a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise" target="_blank" class="logo-item">
                <i class="fab fa-github logo-icon"></i>
                <span class="logo-text">GitHub Repository</span>
              </a>
              <a href="#dataset-table" class="logo-item">
                <i class="fas fa-database logo-icon"></i>
                <span class="logo-text">Training Datasets</span>
              </a>
              <a href="#test-datasets" class="logo-item">
                <i class="fas fa-file-alt logo-icon"></i>
                <span class="logo-text">Test Datasets</span>
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Introduction Section -->
  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Introduction</h2>
      </div>
      <div class="content">
        <p>
          Cross-modal retrieval faces significant challenges from <strong>noisy web-crawled data</strong>, yet current <strong>Noisy Correspondence Learning (NCL) evaluation protocols</strong> rely on unrealistic uniform noise assumptions like <strong>random shuffling</strong>, which fail to capture the structured, <strong>semantics-dependent biases</strong> inherent in real-world mismatches. Unlike <strong>Noisy Label Learning</strong>, which rigorously distinguishes between uniform and instance-dependent errors, NCL has largely overlooked how <strong>systematic annotation and collection biases</strong> create content-specific noise.
        </p>
        <p>
          To bridge this gap, we introduce a <strong>novel benchmark and generation framework</strong>, as illustrated in <strong>Fig. 1</strong>, that leverages <strong>Large Multimodal and Language Models (LMMs and LLMs)</strong> to synthesize <strong>controlled, realistic, and semantics-dependent perturbations</strong>. This approach moves beyond random shuffling to provide a <strong>rigorous evaluation of model robustness</strong> against complex data corruption.
        </p>
        <figure class="image is-fullwidth" style="margin-top: 2rem;">
          <img src="assets/img/four_noise.png" alt="Noise Types" style="border-radius: 8px; box-shadow: 0 4px 10px rgba(0,0,0,0.1);">
          <div class="img-caption-tag">Figure 1: Framework for synthesizing structured semantic-dependent noise in image-text pairs.</div>
        </figure>
      </div>
    </div>
  </section>

  <!-- Noise Datasets Table -->
  <section class="content-section" style="background-color: #fcfcfc;" id="dataset-table">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Noise Datasets</h2>
      </div>
      <div class="content">
        <p>
          The dataset directory contains folders for each type of noise applied to the MS-COCO and Flickr30K datasets. The noise types include:
        </p>
        <ul>
          <li><strong>Entity Referential Error (ER)</strong>: Specific entities in the captions are replaced with semantically related but incorrect terms, creating fine-grained mismatches.</li>
          <li><strong>High-Level Semantic Confusion (SC)</strong>: Captions are swapped from semantically proximate images in feature space.</li>
          <li><strong>Object Omission (OO)</strong>: Captions describe the scene but omit some objects, simulating incomplete annotation.</li>
          <li><strong>Short Description (SD)</strong>: Captions are condensed to generic statements, mimicking low-effort annotations.</li>
        </ul>
        <p>
         To download the captions, first visit:
        </p>
      </div>

      <div class="table-container">
        <table class="table is-fullwidth is-hoverable is-striped">
          <thead>
            <tr>
              <th style="width: 25%;">Dataset</th>
              <th>Flickr30k</th>
              <th>MS-COCO</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>Object Omission</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_f30k/annotations/scan_split/" target="_blank">Object_Omission_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_MSCOCO/annotations/scan_split/" target="_blank">Object_Omission_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Object Omission (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_5error_f30k/annotations/scan_split/" target="_blank">Object_Omission_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Object_Omission_noise_5error_MSCOCO/annotations/scan_split/" target="_blank">Object_Omission_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Short Description</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_f30k/annotations/scan_split/" target="_blank">Short_Description_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_MSCOCO/annotations/scan_split/" target="_blank">Short_Description_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Short Description (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_5error_f30k/annotations/scan_split/" target="_blank">Short_Description_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Short_Description_noise_5error_MSCOCO/annotations/scan_split/" target="_blank">Short_Description_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Entity Referential Error</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_f30k/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_MSCOCO/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_MSCOCO</a></td>
            </tr>
            <tr>
              <td>Entity Referential Error (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_5error_f30k/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/Entity_Referential_Error_noise_5error_MSCOCO/annotations/scan_split/" target="_blank">Entity_Referential_Error_noise_5error_MSCOCO</a></td>
            </tr>
            <tr>
              <td>High-level Semantic Confusion</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_f30k/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_MSCOCO/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_MSCOCO</a></td>
            </tr>
            <tr>
              <td>High-level Semantic Confusion (5error)</td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_5error_f30k/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_5error_f30k</a></td>
              <td><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/tree/main/dataset/High_level_Semantic_Confusion_5error_MSCOCO/annotations/scan_split/" target="_blank">High_level_Semantic_Confusion_5error_MSCOCO</a></td>
            </tr>
          </tbody>
        </table>
      </div>
      <p>
        <p>Then you can access the following training files according to your needs:</p>
        <ul>
          <li>0.2_noise_train_caps.txt</li>
          <li>0.4_noise_train_caps.txt</li>
          <li>0.6_noise_train_caps.txt</li>
          <li>1.0_noise_train_caps.txt</li>
        </ul>
      </p>
    </div>
  </section>

  <!-- New Test Dataset Section -->
  <section class="test-dataset-section" id="test-datasets">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Noisy Test Datasets</h2>
      </div>
      
      <div class="test-dataset-card">
        <h3 class="test-dataset-header">Noisy Test Captions</h3>
        
        <div class="test-dataset-links">
          <div class="test-dataset-link">
            <i class="fas fa-file-alt test-dataset-link-icon"></i>
            <div class="test-dataset-link-text">
              MS-COCO Noisy Test Captions: 
              <a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/blob/main/dataset/test_caps_mix_coco.txt" target="_blank">test_caps_mix_coco.txt</a>
            </div>
          </div>
          
          <div class="test-dataset-link">
            <i class="fas fa-file-alt test-dataset-link-icon"></i>
            <div class="test-dataset-link-text">
              Flickr30K Noisy Test Captions: 
              <a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise/blob/main/dataset/test_caps_mix_f30k.txt" target="_blank">test_caps_mix_f30k.txt</a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- Model Training Methodologies -->
  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Model Training Methodologies</h2>
      </div>
      <div class="content">
        <p>
          We validate the effectiveness of the noise datasets using two primary training strategies:
        </p>
        <ul>
          <li><strong>CLIP Pre-trained Model</strong>: Fine-tuning a pre-trained CLIP model to evaluate direct performance degradation under noisy conditions.</li>
          <li><strong>NPC (Negative Pre-aware for Noisy Cross-modal Matching)</strong>: Estimates sample-specific negative impact via a clean-sample memory bank, adaptively weighting training samples to prevent noise accumulation and ensure stable high-noise performance.</li>
        </ul>
      </div>
    </div>
  </section>

  <!-- Results Analysis Section -->
  <section class="content-section result-section-wrapper">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Results Analysis</h2>
      </div>
      
      <!-- Module 1: Robustness Comparison -->
      <div class="result-card">
        <h3 class="result-header">1. Robustness Comparison: NPC vs. CLIP</h3>
        
        <div class="content">
          <p>
            Our comparisons on <strong>Flickr30k</strong> and <strong>MS-COCO</strong>. 
          </p>
        </div>

        <div class="columns is-variable is-6">
          <div class="column is-half">
            <div class="large-img-container">
               <div class="img-caption-tag">Table 1: Retrieval metrics for Image-to-Text and Text-to-Image on Flickr30K with various noise (CLIP / NPC)</div>
               <img src="assets/img/table1.png" alt="Comparison Table on Flickr30k">
            </div>
          </div>
          <div class="column is-half">
            <div class="large-img-container">
               <div class="img-caption-tag">Table 2: Retrieval metrics for Image-to-Text and Text-to-Image on MSCOCO 5K with various noise(CLIP / NPC)</div>
               <img src="assets/img/table2.png" alt="Comparison Table on MS-COCO">
            </div>
          </div>
        </div>

        <div class="content">

        </div>
      </div>

      <!-- Module 2: Noise Impact & Bias -->
      <div class="result-card">
        <h3 class="result-header">2. Noise Impact on Model Behavior</h3>
        
        <div class="columns is-vcentered">

          <div class="column is-8">
            <div class="large-img-container" style="margin: 0;">
              <img src="assets/img/figure2.png" alt="Average Similarity Score Analysis (Fig. 2)">
              <div class="img-caption-tag">Figure 2: Average Cross-Modal Similarity Scores of
                Noisily Trained Models on Semantic-Dependent Noisy
                Test Sets</div>
            </div>
          </div>
        </div>
      </div>

    </div>
  </section>

  <section class="content-section">
    <div class="container is-max-desktop">
      <div class="section-title">
        <h2>Conclusion</h2>
      </div>
      <div class="content">
        We introduced a semantics-aware benchmark and generation framework for evaluating robustness in noisy imageâ€“text correspondence, moving beyond random shuffling to four systematic, semantic-dependent noise types. Through controlled LMM/LLM re-captioning and feature-space retrieval, we created reproducible perturbations with distinct linguistic and distributional signatures that expose complementary failure modes in cross-modal retrieval.
        Our re-evaluation of representative baselines and NCL methods reveals several key findings: Robustness under random shuffling overestimates true resilience. NCL strategies tuned for unstructured corruption can underperform simple fine-tuning when residual semantic cues persist, especially at high noise ratios, highlighting the need for denoising that preserves weak but useful alignment signals rather than suppressing them.
        Together, these results argue for semantics-aware evaluation as a necessary complement to existing benchmarks. Our benchmark provides a practical, extensible testbed to study structured correspondence noise, enabling more faithful measurement and targeted algorithmic advances.  We envision this benchmark could facilitate a shift toward methods and metrics that match the semantic reality of noisy web-scale supervision.
      </div>
    </div>
  </section>

  <!-- Footer -->
  <footer class="footer-section">
    <div class="container is-max-desktop">
      <div class="columns">
        <div class="column is-half">
          <h3 style="color: #ff8b8b; font-size: 1.4rem; margin-bottom: 1rem;">Semantic-dependent-Corresponding-Noise</h3>
          <p style="opacity: 0.8;">Introduce the first benchmark targeting semantic-dependent noisy correspondence to reveal the limitations of random-shuffling evaluations</p>
        </div>
        <div class="column is-half has-text-right-tablet">
          <h4 style="color: #ff8b8b; margin-bottom: 1rem;">Links</h4>
          <ul style="list-style: none; padding: 0;">
            <li style="margin-bottom: 0.5rem;"><a href="https://github.com/Semantic-dependent-Corresponding-Noise/Semantic-dependent-Corresponding-Noise" style="color: white; opacity: 0.8; text-decoration: none;">GitHub Repository</a></li>
          </ul>
        </div>
      </div>
      <hr style="border-color: rgba(255,255,255,0.1); margin: 2rem 0 1rem;">
      <div class="has-text-centered">
        <p style="opacity: 0.6; font-size: 0.9rem;">&copy; 2025 Semantic-dependent-Corresponding-Noise. All Rights Reserved.</p>
      </div>
    </div>
  </footer>

  <!-- Scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
